<html>
<head>
  <title>Real-Time Emotion Detection</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
  <h1>Emotion Detection</h1>
  <video id="webcam" autoplay></video>
  <canvas id="output"></canvas>
  <script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('output');
    const context = canvas.getContext('2d');

    // Load ONNX model with ONNX Runtime Web
    async function loadModel() {
      try {
        const session = await ort.InferenceSession.create('shufflenetV2_80x80.onnx');
        console.log('Model loaded:', session);
        console.log('Input Names:', session.inputNames); 
        return session;
      } catch (error) {
        console.error('Error loading model:', error);
        throw error;  // Re-throw to propagate the error
      }
    }

    function softmax(logits) {
      const exp = logits.map(x => Math.exp(x));
      const sum = exp.reduce((a, b) => a + b, 0);
      return exp.map(x => x / sum);
    }

    function getPredictedEmotion(output) {
      const probabilities = softmax(output);
      const maxIndex = probabilities.indexOf(Math.max(...probabilities));
      const emotions = ['anger', 'fear', 'happiness', 'neutral', 'sad', 'surprise'];
      return emotions[maxIndex];
    }


    // Setup the webcam
    async function setupWebcam() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        console.log('Webcam initialized');
        return new Promise((resolve) => {
          video.onloadedmetadata = () => {
            resolve(video);
          };
        });
      } catch (error) {
        console.error('Error setting up webcam:', error);
        throw error;  // Re-throw to propagate the error
      }
    }

    async function detectEmotion(session) {
      // Set canvas size to match model input dimensions
      const width = 80;
      const height = 80;
      canvas.width = width;
      canvas.height = height;
    
      // Draw image to canvas and get image data
      context.drawImage(video, 0, 0, width, height);
      const imageData = context.getImageData(0, 0, width, height);
    
      // Convert RGBA to grayscale and create tensor
      const grayData = new Float32Array(width * height);
      let index = 0;
      for (let i = 0; i < imageData.data.length; i += 4) {
        const gray = (imageData.data[i] + imageData.data[i + 1] + imageData.data[i + 2]) / 3;
        grayData[index++] = gray / 255;  // Normalize to [0, 1]
      }
    
      // Create tensor
      const inputName = 'input.1';  // Use the correct input name
      const input = new ort.Tensor('float32', grayData, [1, 1, height, width]);
    
      // Run the model with the input tensor
      try {
        const results = await session.run({ [inputName]: input });
        const outputTensor = results[session.outputNames[0]];
        const outputData = outputTensor.data;
    
        // Process the output
        const emotion = getPredictedEmotion(outputData);
        console.log('Detected Emotion:', emotion);
      } catch (error) {
        console.error('Error during model inference:', error);
      }
    }

    async function run() {
    try {
      const session = await loadModel();  // Load the ONNX model
      console.log('Model loaded successfully');
      
      await setupWebcam();  // Setup webcam for video streaming
      console.log('Webcam setup complete');
      
      setInterval(() => {
        try {
          detectEmotion(session);  // Run the emotion detection function
        } catch (error) {
          console.error('Error during emotion detection:', error);
        }
      }, 100);  // Run detection every 100ms
      
    } catch (error) {
      console.error('Error in main loop:', error);
    }
  }

    run();
  </script>
</body>
</html>
