<html>
<head>
  <title>Real-Time Emotion Detection</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
  <h1>Emotion Detection</h1>
  <video id="webcam" autoplay></video>
  <canvas id="output"></canvas>
  <script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('output');
    const context = canvas.getContext('2d');

    // Load ONNX model with ONNX Runtime Web
    async function loadModel() {
      const session = await ort.InferenceSession.create('model.onnx');
      return session;
    }

    // Setup the webcam
    async function setupWebcam() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    // Run emotion detection using the model
    async function detectEmotion(session) {
      context.drawImage(video, 0, 0, canvas.width, canvas.height);
      const imageData = context.getImageData(0, 0, canvas.width, canvas.height);
      const input = new ort.Tensor('float32', new Float32Array(imageData.data), [1, 3, 80, 80]);

      // Run the model with the input tensor
      const results = await session.run({ input: input });
      const outputTensor = results[session.outputNames[0]];

      // Handle the output here (e.g., display the detected emotion)
      console.log(outputTensor);
    }

    // Main loop
    async function run() {
      const session = await loadModel();
      await setupWebcam();
      setInterval(() => detectEmotion(session), 100);  // Detect emotion every 100ms
    }

    run();
  </script>
</body>
</html>
